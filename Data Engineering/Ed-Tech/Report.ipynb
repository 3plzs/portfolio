{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report for Ed Tech Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() \n",
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))\n",
    "extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "extracted_assessments.registerTempTable('assessments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get our messages from the 'assessments' kafka topic and transform values into strings. We then extract the data from the 'value' portion of our message and create a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "assessments_as_strings =raw_assessments.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further understand what an individual message's contents look like, we cast the assessments as strings and visualize one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "first_assessment=json.loads(assessments_as_strings.select('value').take(1)[0].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_exam_id': '37f0a30a-7464-11e6-aa92-a8667f27e5dc',\n",
       " 'certification': 'false',\n",
       " 'exam_name': 'Normal Forms and All That Jazz Master Class',\n",
       " 'keen_created_at': '1516717442.735266',\n",
       " 'keen_id': '5a6745820eb8ab00016be1f1',\n",
       " 'keen_timestamp': '1516717442.735266',\n",
       " 'max_attempts': '1.0',\n",
       " 'sequences': {'attempt': 1,\n",
       "  'counts': {'all_correct': False,\n",
       "   'correct': 2,\n",
       "   'incomplete': 1,\n",
       "   'incorrect': 1,\n",
       "   'submitted': 4,\n",
       "   'total': 4,\n",
       "   'unanswered': 0},\n",
       "  'id': '5b28a462-7a3b-42e0-b508-09f3906d1703',\n",
       "  'questions': [{'id': '7a2ed6d3-f492-49b3-b8aa-d080a8aad986',\n",
       "    'options': [{'at': '2018-01-23T14:23:24.670Z',\n",
       "      'checked': True,\n",
       "      'correct': True,\n",
       "      'id': '49c574b4-5c82-4ffd-9bd1-c3358faf850d',\n",
       "      'submitted': 1},\n",
       "     {'at': '2018-01-23T14:23:25.914Z',\n",
       "      'checked': True,\n",
       "      'correct': True,\n",
       "      'id': 'f2528210-35c3-4320-acf3-9056567ea19f',\n",
       "      'submitted': 1},\n",
       "     {'checked': False,\n",
       "      'correct': True,\n",
       "      'id': 'd1bf026f-554f-4543-bdd2-54dcf105b826'}],\n",
       "    'user_correct': False,\n",
       "    'user_incomplete': True,\n",
       "    'user_result': 'missed_some',\n",
       "    'user_submitted': True},\n",
       "   {'id': 'bbed4358-999d-4462-9596-bad5173a6ecb',\n",
       "    'options': [{'at': '2018-01-23T14:23:30.116Z',\n",
       "      'checked': True,\n",
       "      'id': 'a35d0e80-8c49-415d-b8cb-c21a02627e2b',\n",
       "      'submitted': 1},\n",
       "     {'checked': False,\n",
       "      'correct': True,\n",
       "      'id': 'bccd6e2e-2cef-4c72-8bfa-317db0ac48bb'},\n",
       "     {'at': '2018-01-23T14:23:41.791Z',\n",
       "      'checked': True,\n",
       "      'correct': True,\n",
       "      'id': '7e0b639a-2ef8-4604-b7eb-5018bd81a91b',\n",
       "      'submitted': 1}],\n",
       "    'user_correct': False,\n",
       "    'user_incomplete': False,\n",
       "    'user_result': 'incorrect',\n",
       "    'user_submitted': True},\n",
       "   {'id': 'e6ad8644-96b1-4617-b37b-a263dded202c',\n",
       "    'options': [{'at': '2018-01-23T14:23:52.510Z',\n",
       "      'checked': False,\n",
       "      'id': 'a9333679-de9d-41ff-bb3d-b239d6b95732'},\n",
       "     {'checked': False, 'id': '85795acc-b4b1-4510-bd6e-41648a3553c9'},\n",
       "     {'at': '2018-01-23T14:23:54.223Z',\n",
       "      'checked': True,\n",
       "      'correct': True,\n",
       "      'id': 'c185ecdb-48fb-4edb-ae4e-0204ac7a0909',\n",
       "      'submitted': 1},\n",
       "     {'at': '2018-01-23T14:23:53.862Z',\n",
       "      'checked': True,\n",
       "      'correct': True,\n",
       "      'id': '77a66c83-d001-45cd-9a5a-6bba8eb7389e',\n",
       "      'submitted': 1}],\n",
       "    'user_correct': True,\n",
       "    'user_incomplete': False,\n",
       "    'user_result': 'correct',\n",
       "    'user_submitted': True},\n",
       "   {'id': '95194331-ac43-454e-83de-ea8913067055',\n",
       "    'options': [{'checked': False,\n",
       "      'id': '59b9fc4b-f239-4850-b1f9-912d1fd3ca13'},\n",
       "     {'checked': False, 'id': '2c29e8e8-d4a8-406e-9cdf-de28ec5890fe'},\n",
       "     {'checked': False, 'id': '62feee6e-9b76-4123-bd9e-c0b35126b1f1'},\n",
       "     {'at': '2018-01-23T14:24:00.807Z',\n",
       "      'checked': True,\n",
       "      'correct': True,\n",
       "      'id': '7f13df9c-fcbe-4424-914f-2206f106765c',\n",
       "      'submitted': 1}],\n",
       "    'user_correct': True,\n",
       "    'user_incomplete': False,\n",
       "    'user_result': 'correct',\n",
       "    'user_submitted': True}]},\n",
       " 'started_at': '2018-01-23T14:23:19.082Z',\n",
       " 'user_exam_id': '6d4089e4-bde5-4a22-b65f-18bce9ab79c8'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the message's contents have a complicated nested schema containing a variety of information relating to the exam and its questions.\n",
    "\n",
    "In order to get an idea of what the data looks like, we run the following simple queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many assessments are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    3280|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from assessments\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3280 assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many people took Learning Git?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     394|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from assessments where exam_name = 'Learning Git'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "394 people took Learning Git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the least common course taken? And the most common?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------+\n",
      "|ct |exam_name                                        |\n",
      "+---+-------------------------------------------------+\n",
      "|1  |Nulls, Three-valued Logic and Missing Information|\n",
      "|1  |Native Web Apps for Android                      |\n",
      "|1  |Learning to Visualize Data with D3.js            |\n",
      "|1  |Operating Red Hat Enterprise Linux Servers       |\n",
      "+---+-------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+---+----------------------+\n",
      "|ct |exam_name             |\n",
      "+---+----------------------+\n",
      "|394|Learning Git          |\n",
      "|162|Introduction to Python|\n",
      "+---+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) as ct, exam_name from assessments group by exam_name order by ct asc \").show(4, truncate=False)\n",
    "\n",
    "spark.sql(\"select count(*) as ct, exam_name from assessments group by exam_name order by ct desc\").show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 exams sharing the last place, all having 1 instance. On the other hand, the \"Learning Git\" class has a firm grip on the first place, superceeding the next class by over 200 instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the average score for each class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps it would yield useful insight regarding an exam's difficulty level to know what its average score is. To more easily deal with this data, I adapt a lambda function to extract from the nested .json a series of columns relating to  exams and their questions. These columns are the exam's name, the total number of questions, the number of correct responses and the number of incomplete responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def my_lambda_correct_total(x):\n",
    "    \n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    \n",
    "    if \"sequences\" in raw_dict:\n",
    "        \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:\n",
    "            \n",
    "            if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"]:\n",
    "                    \n",
    "                my_dict = {\"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"], \n",
    "                           \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"],\n",
    "                          \"exam\": raw_dict[\"exam_name\"],\n",
    "                          \"incomplete\": raw_dict[\"sequences\"][\"counts\"][\"incomplete\"]}\n",
    "                my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list\n",
    "\n",
    "my_correct_total = assessments.rdd.flatMap(my_lambda_correct_total).toDF()\n",
    "\n",
    "my_correct_total.registerTempTable('ct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this data is registered to a temporary table by the name of 'ct' for \"counts\".\n",
    "\n",
    "From this table, I firstly make use of the exam name, the correct, and the total questions counts. Below, I write an SQL query that computes the mean percentage score for each exam and saves it in a means variable. This variable is then saved and made available through parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means = spark.sql(\"select exam, mean(percentage) as mean from (select exam, correct, total, correct/total as percentage from ct order by exam) group by exam order by mean desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+------------------+\n",
      "|exam                                                                   |mean              |\n",
      "+-----------------------------------------------------------------------+------------------+\n",
      "|Nulls, Three-valued Logic and Missing Information                      |1.0               |\n",
      "|Learning to Visualize Data with D3.js                                  |1.0               |\n",
      "|The Closed World Assumption                                            |1.0               |\n",
      "|Learning SQL for Oracle                                                |0.9772727272727273|\n",
      "|Introduction to Java 8                                                 |0.8759493670886073|\n",
      "|Introduction to Amazon Web Services (AWS) - EC2 Deployment Fundamentals|0.8333333333333334|\n",
      "|Introduction to Apache Spark                                           |0.8333333333333334|\n",
      "|Cloud Native Architecture Fundamentals                                 |0.8000000000000003|\n",
      "|Getting Ready for Angular 2                                            |0.8000000000000002|\n",
      "|Understanding the Grails 3 Domain Model                                |0.7857142857142857|\n",
      "|Introduction to Apache Kafka                                           |0.7692307692307693|\n",
      "|Beginning Programming with JavaScript                                  |0.7658227848101266|\n",
      "|Learning Apache Hadoop                                                 |0.765625          |\n",
      "|Refactor a Monolithic Architecture into Microservices                  |0.7647058823529411|\n",
      "|Starting a Grails 3 Project                                            |0.75              |\n",
      "|Using Storytelling to Effectively Communicate Data                     |0.75              |\n",
      "|Git Fundamentals for Web Developers                                    |0.75              |\n",
      "|Introduction to Hadoop YARN                                            |0.75              |\n",
      "|Python Epiphanies                                                      |0.7418300653594773|\n",
      "|Mastering Python - Networking and Security                             |0.74              |\n",
      "+-----------------------------------------------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "means.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "means.write.mode('overwrite').parquet(\"/tmp/means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each exam, when was the earliest and latest start date and how many people took it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps, another question of interest would be the time frame which the exams in our dataset took place. For each class, we see when the earliest and latest exam instances were as well as the number of instances that took place. The SQL query extracts dates from the 'started_at' timestamps in our data.\n",
    "\n",
    "As above, the data is saved to a variable and fed through parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dates = spark.sql(\"select CAST(min(started_at) as DATE) as earliest, CAST(max(started_at) as DATE) as latest, exam_name as exam, count(exam_name) as count from assessments group by exam_name order by earliest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------------------------------------------------+-----+\n",
      "|earliest  |latest    |exam                                                          |count|\n",
      "+----------+----------+--------------------------------------------------------------+-----+\n",
      "|2017-11-21|2018-01-25|Cloud Computing With AWS                                      |17   |\n",
      "|2017-11-21|2018-01-26|Advanced Machine Learning                                     |67   |\n",
      "|2017-11-21|2018-01-09|Collaborating with Git                                        |6    |\n",
      "|2017-11-21|2018-01-23|Git Fundamentals for Web Developers                           |28   |\n",
      "|2017-11-21|2018-01-28|Software Architecture Fundamentals Beyond The Basics          |48   |\n",
      "|2017-11-21|2018-01-25|Introduction to Machine Learning                              |119  |\n",
      "|2017-11-21|2018-01-25|Practical Java Programming                                    |53   |\n",
      "|2017-11-21|2018-01-28|Software Architecture Fundamentals Understanding the Basics   |109  |\n",
      "|2017-11-21|2018-01-25|Learning Eclipse                                              |85   |\n",
      "|2017-11-21|2018-01-25|Mastering Git                                                 |77   |\n",
      "|2017-11-21|2018-01-23|Introduction to Big Data                                      |75   |\n",
      "|2017-11-21|2018-01-24|Learning Linux System Administration                          |59   |\n",
      "|2017-11-21|2018-01-25|Learning Apache Maven                                         |80   |\n",
      "|2017-11-21|2018-01-23|Introduction to Python                                        |162  |\n",
      "|2017-11-21|2018-01-12|Example Exam For Development and Testing oh yeahsdf           |5    |\n",
      "|2017-11-21|2018-01-23|JavaScript Templating                                         |21   |\n",
      "|2017-11-21|2018-01-28|Learning Git                                                  |394  |\n",
      "|2017-11-21|2018-01-28|JavaScript: The Good Parts Master Class with Douglas Crockford|58   |\n",
      "|2017-11-21|2018-01-16|Mastering Advanced Git                                        |34   |\n",
      "|2017-11-21|2018-01-27|Introduction to Java 8                                        |158  |\n",
      "+----------+----------+--------------------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_dates.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "start_dates.write.mode('overwrite').parquet(\"/tmp/start_dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the average number of incomplete questions for every exam and how does this number compare to the total number of questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times, when users find a question to be too challenging they leave it unanswered. Another way to gauge the difficulty level of an exam is to get an idea of how many questions it has and see the average number of questions left unanswered. The query below does exactly this as well as compute the proportional size of the set of unanswered questions with respect to the total number of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete = spark.sql(\"select exam, max(total) as total, mean(incomplete) as incomplete_average, mean(incomplete)/mean(total) as proportional from ct group by exam order by incomplete_average desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------+-----+------------------+-------------------+\n",
      "|exam                                                |total|incomplete_average|proportional       |\n",
      "+----------------------------------------------------+-----+------------------+-------------------+\n",
      "|Arduino Prototyping Techniques                      |6    |2.0               |0.3333333333333333 |\n",
      "|Introduction to Data Science with R                 |7    |1.6744186046511629|0.23920265780730898|\n",
      "|Building Web Services with Java                     |4    |1.6666666666666667|0.4166666666666667 |\n",
      "|I'm a Software Architect, Now What?                 |4    |1.4666666666666666|0.36666666666666664|\n",
      "|Cloud Computing With AWS                            |4    |1.411764705882353 |0.35294117647058826|\n",
      "|Mastering Web Views                                 |4    |1.3333333333333333|0.3333333333333333 |\n",
      "|Learning Java EE 7                                  |3    |1.2               |0.39999999999999997|\n",
      "|Learning to Program with R                          |7    |1.1953125         |0.17075892857142858|\n",
      "|Web & Native Working Together                       |3    |1.125             |0.375              |\n",
      "|Architectural Considerations for Hadoop Applications|5    |1.1               |0.22000000000000003|\n",
      "|Design Patterns in Java                             |5    |1.0666666666666667|0.21333333333333332|\n",
      "|Understanding the Grails 3 Domain Model             |7    |1.0               |0.14285714285714285|\n",
      "|Introduction to Modern Front-End Development        |6    |1.0               |0.16666666666666666|\n",
      "|Client-Side Data Storage for Web Developers         |5    |1.0               |0.2                |\n",
      "|Normal Forms and All That Jazz Master Class         |4    |1.0               |0.25               |\n",
      "|Modeling for Software Architects                    |4    |1.0               |0.25               |\n",
      "|Introduction to Architecting Amazon Web Services    |3    |0.9285714285714286|0.30952380952380953|\n",
      "|Amazon Web Services - Simple Storage Service        |3    |0.9166666666666666|0.3055555555555555 |\n",
      "|The Principles of Microservices                     |4    |0.9090909090909091|0.22727272727272727|\n",
      "|Event-Driven Microservices                          |5    |0.9               |0.18               |\n",
      "+----------------------------------------------------+-----+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "incomplete.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete.write.mode('overwrite').parquet(\"/tmp/incomplete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant Commands from `philpapapolyzos-history.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd project-2-philpapapolyzos/\n",
    "\n",
    "docker-compose up -d\n",
    "\n",
    "docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --\n",
    "replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-philpapapolyzos/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\"  \n",
    "\n",
    "docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
